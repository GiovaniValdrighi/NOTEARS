{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notears_nonlinear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcfusnP78Vdqln0qzS9nmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovaniValdrighi/NOTEARS/blob/master/notears_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUv3o2zBL3av",
        "colab_type": "text"
      },
      "source": [
        "#Notears não-linear\n",
        "Implementação do algoritmo Notears nonlinear para o aprendizado de estruturas (DAG)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQJTo55Xtc5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.optimize as sopt\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYAX007Bzjpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simulate_dag(d, s0, graph_type):\n",
        "    \"\"\"Simulate random DAG with some expected number of edges.\n",
        "    Args:\n",
        "        d (int): num of nodes\n",
        "        s0 (int): expected num of edges\n",
        "        graph_type (str): ER, SF, BP\n",
        "    Returns:\n",
        "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
        "    \"\"\"\n",
        "    def _random_permutation(M):\n",
        "        # np.random.permutation permutes first axis only\n",
        "        P = np.random.permutation(np.eye(M.shape[0]))\n",
        "        return P.T @ M @ P\n",
        "\n",
        "    def _random_acyclic_orientation(B_und):\n",
        "        return np.tril(_random_permutation(B_und), k=-1)\n",
        "\n",
        "    def _graph_to_adjmat(G):\n",
        "        return np.array(G.get_adjacency().data)\n",
        "\n",
        "    if graph_type == 'ER':\n",
        "        # Erdos-Renyi\n",
        "        G_und = ig.Graph.Erdos_Renyi(n=d, m=s0)\n",
        "        B_und = _graph_to_adjmat(G_und)\n",
        "        B = _random_acyclic_orientation(B_und)\n",
        "    elif graph_type == 'SF':\n",
        "        # Scale-free, Barabasi-Albert\n",
        "        G = ig.Graph.Barabasi(n=d, m=int(round(s0 / d)), directed=True)\n",
        "        B = _graph_to_adjmat(G)\n",
        "    elif graph_type == 'BP':\n",
        "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
        "        top = int(0.2 * d)\n",
        "        G = ig.Graph.Random_Bipartite(top, d - top, m=s0, directed=True, neimode=ig.OUT)\n",
        "        B = _graph_to_adjmat(G)\n",
        "    else:\n",
        "        raise ValueError('unknown graph type')\n",
        "    B_perm = _random_permutation(B)\n",
        "    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()\n",
        "    return B_perm\n",
        "\n",
        "def simulate_nonlinear_sem(B, n, sem_type, noise_scale=None):\n",
        "    \"\"\"Simulate samples from nonlinear SEM.\n",
        "    Args:\n",
        "        B (np.ndarray): [d, d] binary adj matrix of DAG\n",
        "        n (int): num of samples\n",
        "        sem_type (str): mlp, mim, gp, gp-add\n",
        "        noise_scale (np.ndarray): scale parameter of additive noise, default all ones\n",
        "    Returns:\n",
        "        X (np.ndarray): [n, d] sample matrix\n",
        "    \"\"\"\n",
        "    def _simulate_single_equation(X, scale):\n",
        "        \"\"\"X: [n, num of parents], x: [n]\"\"\"\n",
        "        z = np.random.normal(scale=scale, size=n)\n",
        "        pa_size = X.shape[1]\n",
        "        if pa_size == 0:\n",
        "            return z\n",
        "        if sem_type == 'mlp':\n",
        "            hidden = 100\n",
        "            W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
        "            W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
        "            W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
        "            W2[np.random.rand(hidden) < 0.5] *= -1\n",
        "            x = sigmoid(X @ W1) @ W2 + z\n",
        "        elif sem_type == 'mim':\n",
        "            w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "            w1[np.random.rand(pa_size) < 0.5] *= -1\n",
        "            w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "            w2[np.random.rand(pa_size) < 0.5] *= -1\n",
        "            w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "            w3[np.random.rand(pa_size) < 0.5] *= -1\n",
        "            x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
        "        elif sem_type == 'gp':\n",
        "            from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "            gp = GaussianProcessRegressor()\n",
        "            x = gp.sample_y(X, random_state=None).flatten() + z\n",
        "        elif sem_type == 'gp-add':\n",
        "            from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "            gp = GaussianProcessRegressor()\n",
        "            x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
        "                     for i in range(X.shape[1])]) + z\n",
        "        else:\n",
        "            raise ValueError('unknown sem type')\n",
        "        return x\n",
        "\n",
        "    d = B.shape[0]\n",
        "    scale_vec = noise_scale if noise_scale else np.ones(d)\n",
        "    X = np.zeros([n, d])\n",
        "    G = ig.Graph.Adjacency(B.tolist())\n",
        "    ordered_vertices = G.topological_sorting()\n",
        "    assert len(ordered_vertices) == d\n",
        "    for j in ordered_vertices:\n",
        "        parents = G.neighbors(j, mode=ig.IN)\n",
        "        X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j])\n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EZlB8JJ3s2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Notears_MLP(tf.keras.models.Model):\n",
        "  '''\n",
        "  Class for the neural network used on NOTEARS non linear model\n",
        "  \n",
        "  Inputs:\n",
        "    dims [int] - list of dimensions of hidden layers, the last dimension must be 1\n",
        "    batch_size [int] - size of the batch for training\n",
        "    bias [bool] - use of bias in model\n",
        "  '''\n",
        "\n",
        "  class bound_adj(tf.keras.constraints.Constraint):\n",
        "    '''Class for fc1 weights constraints, the weights are non-negative and weights on diagonal are 0'''\n",
        "    def __init__(self, n_variables, dims):\n",
        "      self.n_variables = n_variables\n",
        "      self.dims = dims\n",
        "      return\n",
        "    \n",
        "    def __call__(self, w):\n",
        "      w = w * tf.cast(tf.math.greater_equal(w, 0.), tf.float32)\n",
        "      mask = tf.eye(self.n_variables, )\n",
        "      for i in range(self.n_variables):\n",
        "        for m in range(self.dims[1]):\n",
        "          for j in range(self.n_variables):\n",
        "            if i == j:\n",
        "              pass\n",
        "              #w[i + m + j] = 0. #não sei se é valido\n",
        "      return w\n",
        "\n",
        "\n",
        "  def __init__(self, dims, batch_size = 100, bias = True):\n",
        "    super(Notears_MLP, self).__init__()\n",
        "    self.dims = dims\n",
        "    self.n_variables = dims[0]\n",
        "    self.batch_size = batch_size\n",
        "    #fc1 layer [d * m0]\n",
        "    self.fc1_pos = tf.keras.layers.Dense(dims[0] * dims[1], input_shape = (batch_size, dims[0]), use_bias = bias)\n",
        "    self.fc1_neg = tf.keras.layers.Dense(dims[0] * dims[1], input_shape = (batch_size, dims[0]), use_bias = bias)\n",
        "    self.locally = []\n",
        "    for i in range(len(dims) - 2):\n",
        "      #fc2 layers [d, m1, m2]\n",
        "      self.locally.append(tf.keras.layers.LocallyConnected1D(dims[i + 2], 1, input_shape = (batch_size, dims[0], dims[i + 1]), activation = 'sigmoid'))\n",
        "\n",
        "  def layers_shape(self):\n",
        "    '''Utility function for val_and_grad'''\n",
        "    shapes = []\n",
        "    for _ in range(2):\n",
        "      shapes.append(self.fc1_pos.weights[0].shape)\n",
        "      shapes.append(self.fc1_neg.weights[1].shape)\n",
        "    for layer in self.locally:\n",
        "      shapes.append(layer.weights[0].shape)\n",
        "      shapes.append(layer.weights[1].shape)\n",
        "    return shapes\n",
        "\n",
        "  def call(self, inputs):\n",
        "    '''\n",
        "    Forward procedure in the neural network, pass the inputs trought fc1 and fc2 layers\n",
        "\n",
        "    Inputs:\n",
        "      inputs [tensor] - tensor of samples with shape [batch_size, n_variables]\n",
        "\n",
        "    Outputs:\n",
        "      out [tensor] - tensor with shape [batch_size, n_variables]\n",
        "    '''\n",
        "\n",
        "    hid = self.fc1_pos(inputs) - self.fc1_neg(inputs) #[n, d * m0]\n",
        "    out = tf.reshape(hid, (self.batch_size, self.n_variables, -1)) #[n, d, m0]\n",
        "    for layer in self.locally:\n",
        "      out = layer(out)\n",
        "    out = tf.squeeze(out, 2) #[n, d, 1] -> [n, d]\n",
        "    return out\n",
        "\n",
        "  def flat_params(self):\n",
        "    '''\n",
        "    Return flat vector of params to Scipy minimize\n",
        "    Order is: fc1_pos wegiths bias - fc1_neg weights bias - layers weights bias\n",
        "    '''\n",
        "    params = []\n",
        "    params.append(tf.reshape(self.fc1_pos.weights[0], tf.math.reduce_prod(self.fc1_pos.weights[0].shape)))\n",
        "    params.append(self.fc1_pos.weights[1])\n",
        "    params.append(tf.reshape(self.fc1_neg.weights[0], tf.math.reduce_prod(self.fc1_pos.weights[0].shape)))\n",
        "    params.append(self.fc1_neg.weights[1])\n",
        "    for layer in self.locally:\n",
        "      params.append(tf.reshape(layer.weights[0], -1))\n",
        "      params.append(tf.reshape(layer.weights[1], -1))\n",
        "    return tf.concat(params, axis = 0).numpy().astype(np.float64)\n",
        "\n",
        "  def flat_bounds(self):\n",
        "    '''\n",
        "    Return flat vector of bounds to Scipy minimize\n",
        "    Order is: fc1_pos wegiths bias - fc1_neg weights bias - layers weights bias\n",
        "    '''\n",
        "    bounds = []\n",
        "    bounds_fc1 = []\n",
        "    for i in range(self.n_variables):\n",
        "      for m in range(self.dims[1]):\n",
        "        for j in range(self.n_variables):\n",
        "          if i == j:\n",
        "            bounds_fc1.append((0.,0.))\n",
        "          else:\n",
        "            bounds_fc1.append((0., None))\n",
        "    bounds.append(bounds_fc1)\n",
        "    bounds.append([(None, None) for _ in range(tf.math.reduce_prod(self.fc1_pos.weights[1].shape))])\n",
        "    bounds.append(bounds_fc1)\n",
        "    bounds.append([(None, None) for _ in range(tf.math.reduce_prod(self.fc1_pos.weights[1].shape))])\n",
        "    for layer in self.locally:\n",
        "      bounds.append([(None, None) for _ in range(tf.math.reduce_prod(layer.weights[0].shape))])\n",
        "      bounds.append([(None, None) for _ in range(tf.math.reduce_prod(layer.weights[1].shape))])\n",
        "    return sum(bounds, [])\n",
        "\n",
        "  def update_params(self, weights):\n",
        "    '''Function to update parameters from a flat params list'''\n",
        "\n",
        "    shapes = self.layers_shape()\n",
        "    s_flat = []\n",
        "    for s in shapes:\n",
        "      if len(s_flat) > 0:\n",
        "        s_flat.append(np.prod(s) + s_flat[-1])\n",
        "      else:\n",
        "        s_flat.append(np.prod(s))\n",
        "    self.fc1_pos.set_weights([np.reshape(weights[:s_flat[0]], self.fc1_pos.weights[0].shape),\n",
        "                                       np.reshape(weights[s_flat[0]:s_flat[1]], self.fc1_pos.weights[1].shape)])\n",
        "    self.fc1_neg.set_weights([np.reshape(weights[s_flat[1]:s_flat[2]], self.fc1_neg.weights[0].shape),\n",
        "                                       np.reshape(weights[s_flat[2]:s_flat[3]], self.fc1_neg.weights[1].shape)])\n",
        "    k = 3\n",
        "    for layer in self.locally:\n",
        "      layer.set_weights([np.reshape(weights[s_flat[k]:s_flat[k+1]], layer.weights[0].shape),\n",
        "                                  np.reshape(weights[s_flat[k+1]:s_flat[k+2]], layer.weights[1].shape)])\n",
        "      k+= 2\n",
        "\n",
        "\n",
        "  def _h(self):\n",
        "    '''Calculate the constraint of fc1 to ensure that it's a DAG'''\n",
        "    fc1_weights = self.fc1_pos.weights[0] - self.fc1_neg.weights[0]\n",
        "    fc1_weights = tf.reshape(fc1_weights, (self.n_variables, -1, self.n_variables))\n",
        "    A = tf.transpose(tf.math.reduce_sum(fc1_weights, axis = 1))\n",
        "    #(Yu et al. 2019 DAG-GNN)\n",
        "    # h(w) = tr[(I + kA*A)^n_variables] - n_variables\n",
        "    M = tf.eye(self.n_variables, num_columns = self.n_variables) + A/self.n_variables\n",
        "    E = tf.pow(M, self.n_variables - 1)\n",
        "    h = tf.math.reduce_sum(tf.transpose(E) * M) - self.n_variables\n",
        "    return h\n",
        "  \n",
        "  def _l2_loss(self):\n",
        "    '''Calculate L2 loss from model parameters'''\n",
        "    loss = 0\n",
        "    fc1_weights = self.fc1_pos.weights[0] - self.fc1_neg.weights[0]\n",
        "    loss +=  tf.math.reduce_sum(tf.pow(fc1_weights, 2))\n",
        "    for layer in self.locally:\n",
        "      loss += tf.math.reduce_sum(tf.pow(layer.weights[0], 2))\n",
        "    return loss\n",
        "\n",
        "  def _l1_loss(self):\n",
        "    '''Calculate L1 loss from fc1 parameters'''\n",
        "    return tf.math.reduce_sum(self.fc1_pos.weights[0] + self.fc1_neg.weights[0])\n",
        "\n",
        "  def to_adj(self):\n",
        "    '''Reshape fc1 to an adjacency matrix'''\n",
        "    fc1_weights = self.fc1_pos.weights[0] - self.fc1_neg.weights[0] #[d, d * m0]\n",
        "    fc1_weights = tf.reshape(fc1_weights, (self.n_variables, -1, self.n_variables)) #[d, m0, d]\n",
        "    return tf.transpose(tf.math.reduce_sum(fc1_weights, axis = 1)) #[d, d]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x31bqbkGdk0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def notears_nonlinear(dims, X,  h_tol = 1e-4, threshold = 0.2, lambda1 = 0.5, lambda2 = 0.5, rho_max = 1e+20, max_iter = 1e+16):\n",
        "  '''\n",
        "    Function that apply the NOTEARS algorithm in a non linear model\n",
        "    \n",
        "    Args:\n",
        "        dims (int) : list of dimensions for neural network\n",
        "        X (numpy.matrix) : [n_samples, n_variables] samples matrix \n",
        "        h_tol (float) : tolerance for constraint, exit condition \n",
        "        threshold (float) : threshold for W_est edge values\n",
        "        lambda1 (float) : L1 regularization parameter\n",
        "        lambda2 (float) : L2 regularization parameter \n",
        "        rho_max (float) : max value for rho in augmented lagrangian\n",
        "        max_iter (int) : max number of iterations\n",
        "    Outputs:\n",
        "        W_est (numpy.matrix): [n_variables, n_variables] estimated graph\n",
        "    '''\n",
        "  def square_loss(X, Y):\n",
        "    '''Calculate mean square error from X Y'''\n",
        "    n = X.shape[0]\n",
        "    loss = 0.5 * tf.math.reduce_sum(tf.pow(X -Y, 2)) / n\n",
        "    return tf.cast(loss, tf.float32)\n",
        "\n",
        "  def val_and_grad(flat_params):\n",
        "    '''Calculate loss value and gradient for Scipy optmize'''\n",
        "    with tf.GradientTape() as tape:\n",
        "      Y = model(X).numpy()\n",
        "      mse_loss = square_loss(X, Y) \n",
        "      h = model._h()\n",
        "      h_constraint = 0.5 * rho * h * h + alpha * h\n",
        "      fc1_loss = lambda1 * model._l1_loss()\n",
        "      locally_loss = 0.5 * lambda2 * model._l2_loss()\n",
        "      loss = mse_loss + h_constraint + fc1_loss + locally_loss\n",
        "    grad = tape.gradient(loss, model.trainable_variables)\n",
        "    flat_grad = []\n",
        "    i = 1\n",
        "    for gradient in grad:\n",
        "      if gradient != None:\n",
        "        flat_grad.append(tf.reshape(gradient, -1).numpy())\n",
        "      else:\n",
        "        flat_grad.append(np.array([0 for _ in range(tf.math.reduce_prod(model.layers_shape()[i]))]))\n",
        "        i+=2\n",
        "    flat_grad = np.concatenate(flat_grad)\n",
        "    return loss.numpy().astype(np.float64), flat_grad\n",
        "    \n",
        "  ########################\n",
        "  # Optimization process #\n",
        "  ########################\n",
        "  model = Notears_MLP(dims, X.shape[0])\n",
        "  _ = model(X)\n",
        "  rho, alpha, h = 1., 0., np.inf\n",
        "  for _ in range(int(max_iter)):\n",
        "    h_new = None\n",
        "    while rho < rho_max:\n",
        "      sol = sopt.minimize(val_and_grad, model.flat_params(), method = \"L-BFGS-B\", jac = True, bounds = model.flat_bounds())\n",
        "      new_params = sol.x\n",
        "      model.update_params(new_params.astype(np.float32))\n",
        "      h_new = model._h().numpy()\n",
        "\n",
        "      #Updating rho constraint parameter\n",
        "      if h_new > h * 0.25:\n",
        "        rho = rho * 10\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "    h = h_new\n",
        "\n",
        "    #Ascent alpha\n",
        "    alpha += rho * h\n",
        "\n",
        "    #Verifying constraint tolerance\n",
        "    if h < h_tol or rho >= rho_max:\n",
        "      break\n",
        "\n",
        "  #Applying threshold   \n",
        "  W_est = model.to_adj().numpy()\n",
        "  W_est[np.abs(W_est) < threshold] = 0\n",
        "  return W_est  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QGr3eya0WJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dag_est = notears_nonlinear([5, 10, 1], X.astype(np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHFvBR_Nv1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ddda43e8-b032-48ff-d35d-70423cae6457"
      },
      "source": [
        "np.transpose(dag_est)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.31067073,  0.        , -0.53133166],\n",
              "       [-0.65230167,  0.        ,  0.26083845,  0.        ,  0.        ],\n",
              "       [ 0.5321901 ,  0.25866973,  0.        ,  0.22993842, -0.370006  ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [-1.414167  ,  0.        ,  0.45628956,  0.        ,  0.        ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOPEFFHB0G-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f801a55f-9b4b-43b8-eab1-4dce76fd0f1e"
      },
      "source": [
        "G_true"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [1., 0., 1., 1., 1.],\n",
              "       [0., 0., 0., 1., 1.],\n",
              "       [1., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXJrUDBhzoMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import expit as sigmoid\n",
        "import igraph as ig\n",
        "import random\n",
        "G_true = simulate_dag(5, 9, 'ER')\n",
        "X = simulate_nonlinear_sem(G_true, 200, 'mim')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Ym_58f0vSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Notears_MLP([5, 10, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLDV89HY2KwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "985b8ca4-8fbd-4bb1-9f41-98c4a3b36d46"
      },
      "source": [
        "out = model(X.astype(np.float32))\n",
        "model.fc1_pos.weights[0].dtype"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rWO1UIBEpou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edeb02ed-b00a-4a25-be56-0f86ae34e7da"
      },
      "source": [
        "model.locally[0].weights[0].dtype"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImeDcwXAFeoI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "091e5a5e-415a-477b-c747-a736a2b971b0"
      },
      "source": [
        "model.fc1_pos.weights"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'notears_mlp_28/dense_56/kernel:0' shape=(5, 50) dtype=float64, numpy=\n",
              " array([[-0.16499503, -0.30215633,  0.06343339,  0.07965161,  0.14066979,\n",
              "         -0.13873502, -0.18235779, -0.124769  ,  0.30861387,  0.28699815,\n",
              "          0.2386138 ,  0.11993983,  0.15438841,  0.192188  ,  0.05114096,\n",
              "          0.05632323, -0.27395713, -0.0367157 , -0.30350054,  0.05087925,\n",
              "         -0.31410413,  0.23411285,  0.17240236, -0.01798805,  0.31396076,\n",
              "         -0.155281  ,  0.19146078,  0.05078613,  0.25499627,  0.18407879,\n",
              "         -0.32438872,  0.03940999, -0.01468688,  0.26728438,  0.27723504,\n",
              "          0.30798503, -0.28224893,  0.25932945, -0.05096385, -0.15652426,\n",
              "         -0.02924489, -0.29804994,  0.27823251, -0.13527119, -0.05858691,\n",
              "          0.1474097 , -0.16917319, -0.13944883, -0.24102308,  0.08237474],\n",
              "        [-0.14934635,  0.06184217, -0.32488558, -0.08121349,  0.04778818,\n",
              "          0.11236055, -0.01568407,  0.22147425,  0.03264974, -0.19912984,\n",
              "         -0.01361658, -0.03419719, -0.01107908,  0.11120218, -0.26443961,\n",
              "         -0.10538099,  0.02581484,  0.15460854, -0.00209961,  0.01421005,\n",
              "         -0.17107267,  0.28829497, -0.21684999, -0.04635711, -0.21353932,\n",
              "          0.21050295, -0.10311281, -0.10356648,  0.21252794, -0.02020026,\n",
              "          0.27403269,  0.26845057,  0.08743467,  0.11636029,  0.09732352,\n",
              "          0.16090515,  0.23207351, -0.00492606, -0.2094782 ,  0.08609828,\n",
              "          0.26716966, -0.22254383,  0.18942756, -0.25740284, -0.21217949,\n",
              "         -0.11163665,  0.15037189, -0.0598832 , -0.03189311,  0.31222964],\n",
              "        [-0.15113935, -0.20922307, -0.10349489, -0.08987959, -0.05824409,\n",
              "          0.02994623, -0.29229164,  0.20398329,  0.10172629, -0.1215786 ,\n",
              "         -0.03528851, -0.02803969, -0.00450224, -0.20469973, -0.11737859,\n",
              "         -0.13785651,  0.10365562, -0.06289805, -0.26157668, -0.14947365,\n",
              "         -0.0743453 ,  0.09863644,  0.06740415, -0.23633358,  0.14047699,\n",
              "         -0.00621732,  0.25035507, -0.18903624, -0.10203053,  0.03423622,\n",
              "         -0.09693561,  0.19527553,  0.2848724 ,  0.24842781,  0.27388063,\n",
              "          0.15474409, -0.16406798,  0.07563071,  0.12984829, -0.28837816,\n",
              "         -0.06352366,  0.12845349,  0.00783775, -0.31902007, -0.31053418,\n",
              "          0.2897476 , -0.21233928,  0.09294172, -0.092725  ,  0.26249675],\n",
              "        [ 0.11021682,  0.16241406,  0.00140381, -0.02733245,  0.08049999,\n",
              "          0.20149883,  0.08503268, -0.28135466,  0.05161762, -0.22790509,\n",
              "          0.12093093,  0.25153534, -0.23861803,  0.21850015, -0.1210943 ,\n",
              "         -0.30324962, -0.15036738,  0.11719635, -0.17362865,  0.2043303 ,\n",
              "          0.1066217 , -0.07976314,  0.05798291,  0.04684396, -0.11866366,\n",
              "         -0.04599242,  0.07220862, -0.04132094, -0.11176853,  0.32712295,\n",
              "         -0.26193019, -0.28596046, -0.23458774,  0.13618879, -0.04887863,\n",
              "         -0.08383603, -0.28474127, -0.22437215,  0.18658458,  0.06054206,\n",
              "          0.14384576, -0.05397548,  0.01913565, -0.3023884 , -0.05405873,\n",
              "         -0.00701356,  0.05849574, -0.26025725, -0.25373099,  0.24120769],\n",
              "        [ 0.18159361,  0.25749222, -0.32803405, -0.05309171, -0.13466746,\n",
              "          0.01306998, -0.27642579, -0.32776909, -0.05993526, -0.12932184,\n",
              "         -0.26337702, -0.30443117, -0.07235816,  0.12037999, -0.0014502 ,\n",
              "          0.04504892, -0.11951344, -0.10905724,  0.10630035, -0.22351124,\n",
              "         -0.1578238 ,  0.2049578 , -0.14122441,  0.0697212 , -0.05879237,\n",
              "         -0.2475543 , -0.23460287,  0.19328277, -0.00846573, -0.01994712,\n",
              "          0.2481919 ,  0.2476694 ,  0.14787157, -0.0787257 , -0.12288932,\n",
              "          0.30645696, -0.17215704,  0.20169968,  0.06902858,  0.05592045,\n",
              "          0.32579356,  0.22358728,  0.26817322, -0.20056142, -0.19819283,\n",
              "          0.15503177, -0.2605387 , -0.14198328, -0.08360551,  0.28345317]])>,\n",
              " <tf.Variable 'notears_mlp_28/dense_56/bias:0' shape=(50,) dtype=float64, numpy=\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkrgEL9tGnpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2986098-ce5c-4e2e-eaf8-837315ff60e7"
      },
      "source": [
        "X.dtype"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1e9vP-RGvsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}