{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notears_nonlinear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+hSgUbwKRDDM6FLOXWPBW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovaniValdrighi/NOTEARS/blob/master/notears_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUv3o2zBL3av",
        "colab_type": "text"
      },
      "source": [
        "#Notears não-linear\n",
        "Implementação do algoritmo Notears nonlinear para o aprendizado de estruturas (DAG)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQJTo55Xtc5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a688f62-39ec-4a29-fb04-a7ac3e989b7e"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.optimize as sopt\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnCT1iaayAzk",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/59029854/use-scipy-optimizer-with-tensorflow-2-0-for-neural-network-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EZlB8JJ3s2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Notears_MLP(tf.keras.models.Model):\n",
        "  '''\n",
        "  Class for the neural network used on NOTEARS non linear model\n",
        "  \n",
        "  Inputs:\n",
        "    dims [int] - list of dimensions of hidden layers, the last dimension must be 1\n",
        "    batch_size [int] - size of the batch for training\n",
        "    bias [bool] - use of bias in model\n",
        "  '''\n",
        "\n",
        "  class bound_adj(tf.keras.constraints.Constraint):\n",
        "    '''Class for fc1 weights constraints, the weights are non-negative and weights on diagonal are 0'''\n",
        "    def __init__(self, n_variables, dims):\n",
        "      self.n_variables = n_variables\n",
        "      self.dims = dims\n",
        "      return\n",
        "    \n",
        "    def __call__(self, w):\n",
        "      w = w * tf.cast(tf.math.greater_equal(w, 0.), tf.float32)\n",
        "      mask = tf.eye(self.n_variables, )\n",
        "      for i in range(self.n_variables):\n",
        "        for m in range(self.dims[1]):\n",
        "          for j in range(self.n_variables):\n",
        "            if i == j:\n",
        "              pass\n",
        "              #w[i + m + j] = 0. #não sei se é valido\n",
        "      return w\n",
        "\n",
        "\n",
        "  def __init__(self, dims, batch_size = 100, bias = True):\n",
        "    super(Notears_MLP, self).__init__()\n",
        "    self.dims = dims\n",
        "    self.n_variables = dims[0]\n",
        "    self.batch_size = batch_size\n",
        "    #fc1 layer [d * m0]\n",
        "    self.fc1_pos = tf.keras.layers.Dense(dims[0] * dims[1], input_shape = (batch_size, dims[0]), use_bias = bias)\n",
        "    self.fc1_neg = tf.keras.layers.Dense(dims[0] * dims[1], input_shape = (batch_size, dims[0]), use_bias = bias)\n",
        "    self.locally = []\n",
        "    for i in range(len(dims) - 2):\n",
        "      #fc2 layers [d, m1, m2]\n",
        "      self.locally.append(tf.keras.layers.LocallyConnected1D(dims[i + 2], 1, input_shape = (batch_size, dims[0], dims[i + 1]), activation = 'sigmoid'))\n",
        "\n",
        "  def bias_shape(self):\n",
        "    '''Utility function for val_and_grad'''\n",
        "    res = []\n",
        "    res.append(self.fc1_pos.weights[1].shape)\n",
        "    res.append(self.fc1_neg.weights[1].shape)\n",
        "    for layer in self.locally:\n",
        "      res.append(layer.weights[1].shape)\n",
        "    return res\n",
        "\n",
        "  def call(self, inputs):\n",
        "    '''\n",
        "    Forward procedure in the neural network, pass the inputs trought fc1 and fc2 layers\n",
        "\n",
        "    Inputs:\n",
        "      inputs [tensor] - tensor of samples with shape [batch_size, n_variables]\n",
        "\n",
        "    Outputs:\n",
        "      out [tensor] - tensor with shape [batch_size, n_variables]\n",
        "    '''\n",
        "\n",
        "    hid = self.fc1_pos(inputs) - self.fc1_neg(inputs) #[n, d * m0]\n",
        "    out = tf.reshape(hid, (self.batch_size, self.n_variables, -1)) #[n, d, m0]\n",
        "    for layer in self.locally:\n",
        "      out = layer(out)\n",
        "    out = tf.squeeze(out, 2) #[n, d, 1] -> [n, d]\n",
        "    return out\n",
        "\n",
        "  def flat_params(self):\n",
        "    '''\n",
        "    Return flat vector of params to Scipy minimize\n",
        "    Order is: fc1_pos wegiths bias - fc1_neg weights bias - layers weights bias\n",
        "    '''\n",
        "    params = []\n",
        "    params.append(tf.reshape(self.fc1_pos.weights[0], 80))\n",
        "    params.append(self.fc1_pos.weights[1])\n",
        "    params.append(tf.reshape(self.fc1_neg.weights[0], 80))\n",
        "    params.append(self.fc1_neg.weights[1])\n",
        "    for layer in self.locally:\n",
        "      params.append(tf.reshape(layer.weights[0], -1))\n",
        "      params.append(tf.reshape(layer.weights[1], -1))\n",
        "    return tf.cast(tf.concat(params, axis = 0), tf.float64).numpy()\n",
        "\n",
        "  def flat_bounds(self):\n",
        "    '''\n",
        "    Return flat vector of bounds to Scipy minimize\n",
        "    Order is: fc1_pos wegiths bias - fc1_neg weights bias - layers weights bias\n",
        "    '''\n",
        "    bounds = []\n",
        "    bounds_fc1 = []\n",
        "    for i in range(self.n_variables):\n",
        "      for m in range(self.dims[1]):\n",
        "        for j in range(self.n_variables):\n",
        "          if i == j:\n",
        "            bounds_fc1.append((0,0))\n",
        "          else:\n",
        "            bounds_fc1.append((0, None))\n",
        "    bounds.append(bounds_fc1)\n",
        "    bounds.append([(None, None) for _ in range(20)])\n",
        "    bounds.append(bounds_fc1)\n",
        "    bounds.append([(None, None) for _ in range(20)])\n",
        "    for layer in self.locally:\n",
        "      bounds.append([(None, None) for _ in range(tf.math.reduce_prod(layer.weights[0].shape))])\n",
        "      bounds.append([(None, None) for _ in range(tf.math.reduce_prod(layer.weights[1].shape))])\n",
        "    return sum(bounds, [])\n",
        "\n",
        "  def _h(self):\n",
        "    '''Calculate the constraint of fc1 to ensure that it's a DAG'''\n",
        "    fc1_weights = self.fc1_pos.weights[0] - self.fc1_neg.weights[0]\n",
        "    fc1_weights = tf.reshape(fc1_weights, (self.n_variables, -1, self.n_variables))\n",
        "    A = tf.transpose(tf.math.reduce_sum(fc1_weights, axis = 1))\n",
        "    #(Yu et al. 2019 DAG-GNN)\n",
        "    # h(w) = tr[(I + kA*A)^n_variables] - n_variables\n",
        "    M = tf.eye(self.n_variables, num_columns = self.n_variables) + A/self.n_variables\n",
        "    E = tf.pow(M, self.n_variables - 1)\n",
        "    h = tf.math.reduce_sum(tf.transpose(E) * M) - self.n_variables\n",
        "    return h\n",
        "  \n",
        "  def _l2_loss(self):\n",
        "    '''Calculate L2 loss from model parameters'''\n",
        "    loss = 0\n",
        "    fc1_weights = self.fc1_pos.weights[0] - self.fc1_neg.weights[0]\n",
        "    loss +=  tf.math.reduce_sum(tf.pow(fc1_weights, 2))\n",
        "    for layer in self.locally:\n",
        "      loss += tf.math.reduce_sum(tf.pow(layer.weights[0], 2))\n",
        "    return loss\n",
        "\n",
        "  def _l1_loss(self):\n",
        "    '''Calculate L1 loss from fc1 parameters'''\n",
        "    return tf.math.reduce_sum(self.fc1_pos.weights[0] + self.fc1_neg.weights[0])\n",
        "\n",
        "  def to_adj(self):\n",
        "    '''Reshape fc1 to an adjacency matrix'''\n",
        "    fc1_weights = self.fc1_pos.weights[0] - self.fc1_neg.weights[0] #[d, d * m0]\n",
        "    fc1_weights = tf.reshape(fc1_weights, (self.n_variables, -1, self.n_variables)) #[d, m0, d]\n",
        "    return tf.transpose(tf.math.reduce_sum(fc1_weights, axis = 1)) #[d, d]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzpit1vn-GkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.ones((100, 4))\n",
        "data[:, 1] = data[:, 1] * 5 + np.random.randint(0, 10, size = (100))\n",
        "data_tf = tf.constant(data, dtype = np.float32)\n",
        "\n",
        "model = Notears_MLP([4, 5, 7, 1])\n",
        "out = model(data_tf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x31bqbkGdk0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def notears_nonlinear(dims, X,  h_tol = 1e-4, threshold = 0.2, lambda1 = 0.5, lambda2 = 0.5, rho_max = 1e20, max_iter = 1e16):\n",
        "  '''\n",
        "    Function that apply the NOTEARS algorithm in a non linear model\n",
        "    \n",
        "    Args:\n",
        "        dims (int) : list of dimensions for neural network\n",
        "        X (numpy.matrix) : [n_samples, n_variables] samples matrix \n",
        "        h_tol (float) : tolerance for constraint, exit condition \n",
        "        threshold (float) : threshold for W_est edge values\n",
        "        lambda1 (float) : L1 regularization parameter\n",
        "        lambda2 (float) : L2 regularization parameter \n",
        "        rho_max (float) : max value for rho in augmented lagrangian\n",
        "        max_iter (int) : max number of iterations\n",
        "    Outputs:\n",
        "        W_est (numpy.matrix): [n_variables, n_variables] estimated graph\n",
        "    '''\n",
        "  def square_loss(X, Y):\n",
        "    '''Calculate mean square error from X Y'''\n",
        "    n = X.shape[0]\n",
        "    loss = 0.5 * tf.math.reduce_sum(tf.pow(X -Y, 2)) / n\n",
        "    return loss\n",
        "\n",
        "  def val_and_grad():\n",
        "    '''Calculate loss value and gradient for Scipy optmize'''\n",
        "    with tf.GradientTape() as tape:\n",
        "      Y = model(X).numpy()\n",
        "      mse_loss = square_loss(X, Y) \n",
        "      h = model._h()\n",
        "      h_constraint = 0.5 * rho * h * h + alpha * h\n",
        "      fc1_loss = lambda1 * model._l1_loss()\n",
        "      locally_loss = 0.5 * lambda2 * model._l2_loss()\n",
        "      loss = mse_loss + h_constraint + fc1_loss + locally_loss\n",
        "    grad = tape.gradient(loss, model.trainable_variables)\n",
        "    flat_grad = []\n",
        "    i = 0\n",
        "    for gradient in grad:\n",
        "      if gradient != None:\n",
        "        flat_grad.append(tf.reshape(gradient, -1).numpy())\n",
        "      else:\n",
        "        flat_grad.append(np.array([None for _ in range(tf.math.reduce_prod(model.bias_shape()[i]))]))\n",
        "        i+=1\n",
        "    return loss, grad_flat\n",
        "\n",
        "  ########################\n",
        "  # Optimization process #\n",
        "  ########################\n",
        "  model = Notears_MLP(dims)\n",
        "  rho, alpha, h = 0., 0., np.inf\n",
        "  for _ in range(max_iter):\n",
        "    h_new = None\n",
        "    while rho < rho_max:\n",
        "      sol = sopt.minimize(val_and_grad, model.flat_params(), method = \"L-BFGS-B\", jac = True, bounds = model.flat_bounds())\n",
        "      new_params = sol.x\n",
        "      model.update_params(new_params)#fazer\n",
        "      h_new = model._h().numpy()\n",
        "\n",
        "      #Updating rho constraint parameter\n",
        "      if h_new > h * c:\n",
        "        rho = rho * 10\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "    h = h_new\n",
        "\n",
        "    #Ascent alpha\n",
        "    alpha += rho * h\n",
        "\n",
        "    #Verifying constraint tolerance\n",
        "    if h < h_tol or rho >= rho_max:\n",
        "      break\n",
        "\n",
        "  #Applying threshold   \n",
        "  W_est = model.to_adj().numpy()\n",
        "  W_est[np.abs(W_est) < threshold] = 0\n",
        "  return W_est  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}